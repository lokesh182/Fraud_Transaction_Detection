{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "ERJUcMC1cVVQ",
        "outputId": "78fe9e63-2436-4b36-a8dd-4374b337afbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m851.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5f75914c83cb>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls /content/drive/My\\\\ Drive/Colab/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Fraud Transaction Detection\n",
        "\n",
        "# Library Imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "!pip install category_encoders\n",
        "from category_encoders import WOEEncoder\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import resample\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "sns.set_style('whitegrid')  # Set the Seaborn plot style to 'whitegrid'\n",
        "sns.set_palette('pastel')   # Set the color palette to 'pastel'\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
        "\n",
        "# Data Loading\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls /content/drive/My\\ Drive/Colab/\n",
        "\n",
        "file_path_train = '/content/drive/My Drive/Colab/fraudTrain.csv'\n",
        "file_path_test = '/content/drive/My Drive/Colab/fraudTest.csv'\n",
        "\n",
        "train_df = pd.read_csv(file_path_train, index_col='Unnamed: 0')\n",
        "test_df = pd.read_csv(file_path_test, index_col='Unnamed: 0')\n",
        "\n",
        "print(train_df.head(3))\n",
        "print(train_df.info())\n",
        "print(f\"Shape of training data: {train_df.shape}\")\n",
        "\n",
        "fraud_counts = train_df[\"is_fraud\"].value_counts()\n",
        "print(f\"Fraudulent Transactions: {fraud_counts[1]}\")\n",
        "print(f\"Non-Fraudulent Transactions: {fraud_counts[0]}\")\n",
        "\n",
        "print(f\"Total missing values: {train_df.isna().sum().sum()}\")\n",
        "print(f\"Total duplicate rows: {train_df.duplicated().sum()}\")\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 8))\n",
        "\n",
        "# Gender Distribution\n",
        "explode = [0.1, 0.1]\n",
        "train_df.groupby('gender')['is_fraud'].count().plot.pie(explode=explode, autopct=\"%1.1f%%\", ax=ax[0])\n",
        "\n",
        "# Count plot\n",
        "gender_plot = sns.countplot(x=\"gender\", hue=\"is_fraud\", data=train_df, ax=ax[1])\n",
        "for patch in gender_plot.patches:\n",
        "    gender_plot.annotate(f'{patch.get_height()}', (patch.get_x() + patch.get_width() / 2., patch.get_height()),\n",
        "                         ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "\n",
        "plt.title(\"Gender Distribution with Fraud Status\")\n",
        "plt.xlabel(\"Gender\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Distribution of fraud cases: {fraud_counts}\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pie(fraud_counts, labels=[\"No Fraud\", \"Fraud\"], autopct=\"%0.0f%%\")\n",
        "plt.title(\"Fraud Distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"The dataset is highly imbalanced, with 99% non-fraudulent transactions.\")\n",
        "\n",
        "# Feature Engineering\n",
        "\n",
        "# Convert date columns to datetime\n",
        "train_df['trans_date_trans_time'] = pd.to_datetime(train_df['trans_date_trans_time'])\n",
        "test_df['trans_date_trans_time'] = pd.to_datetime(test_df['trans_date_trans_time'])\n",
        "\n",
        "# Extract hour and month\n",
        "train_df['hour'] = train_df['trans_date_trans_time'].dt.hour\n",
        "test_df['hour'] = test_df['trans_date_trans_time'].dt.hour\n",
        "\n",
        "train_df['month'] = train_df['trans_date_trans_time'].dt.month\n",
        "test_df['month'] = test_df['trans_date_trans_time'].dt.month\n",
        "\n",
        "print(train_df.head())\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
        "\n",
        "ax1 = sns.histplot(x='hour', data=train_df[train_df[\"is_fraud\"] == 0],\n",
        "                   stat=\"density\", bins=24, ax=ax1, color=\"orange\")\n",
        "\n",
        "ax2 = sns.histplot(x='hour', data=train_df[train_df[\"is_fraud\"] == 1],\n",
        "                   stat=\"density\", bins=24, ax=ax2, color=\"green\")\n",
        "\n",
        "ax1.set_title(\"Non-Fraudulent Transactions\")\n",
        "ax2.set_title(\"Fraudulent Transactions\")\n",
        "\n",
        "ax1.set_xticks(np.arange(24))\n",
        "ax2.set_xticks(np.arange(24))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Fraudulent transactions are predominantly occurring around midnight.\")\n",
        "\n",
        "unique_transactions = len(train_df['trans_num'].unique())\n",
        "print(f\"Unique transaction count: {unique_transactions}\")\n",
        "\n",
        "# Drop irrelevant columns\n",
        "columns_to_remove = ['first', 'unix_time', 'dob', 'cc_num', 'zip', 'city', 'street', 'state', 'trans_num', 'trans_date_trans_time']\n",
        "train_df.drop(columns=columns_to_remove, inplace=True)\n",
        "test_df.drop(columns=columns_to_remove, inplace=True)\n",
        "\n",
        "# Clean merchant names\n",
        "train_df['merchant'] = train_df['merchant'].str.replace('fraud_', '')\n",
        "\n",
        "print(train_df.info())\n",
        "print(train_df.head(2))\n",
        "\n",
        "# Data Encoding\n",
        "\n",
        "# Apply label encoding to 'gender'\n",
        "train_df['gender'] = train_df['gender'].map({'F': 0, 'M': 1})\n",
        "\n",
        "# Apply Weight of Evidence (WOE) encoding\n",
        "encoder = WOEEncoder()\n",
        "for column in ['job', 'merchant', 'category', 'lat', 'last']:\n",
        "    train_df[column] = encoder.fit_transform(train_df[column], train_df['is_fraud'])\n",
        "\n",
        "print(train_df.head(3))\n",
        "\n",
        "# Resampling to address class imbalance\n",
        "\n",
        "non_fraud_class = train_df[train_df[\"is_fraud\"] == 0]\n",
        "fraud_class = train_df[train_df[\"is_fraud\"] == 1]\n",
        "\n",
        "non_fraud_class_downsampled = resample(non_fraud_class, replace=False, n_samples=len(fraud_class))\n",
        "balanced_data = pd.concat([fraud_class, non_fraud_class_downsampled])\n",
        "\n",
        "X = balanced_data.drop(\"is_fraud\", axis=1)\n",
        "y = balanced_data[\"is_fraud\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=65)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Display class distributions\n",
        "original_counts = train_df[\"is_fraud\"].value_counts()\n",
        "downsampled_counts = balanced_data[\"is_fraud\"].value_counts()\n",
        "\n",
        "original_percentages = original_counts / len(train_df) * 100\n",
        "downsampled_percentages = downsampled_counts / len(balanced_data) * 100\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Original class distribution\n",
        "plt.subplot(1, 2, 1)\n",
        "bars = plt.bar(original_counts.index, original_counts.values, color=['orange', 'green'])\n",
        "for bar, label in zip(bars, original_percentages):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5, f'{label:.2f}%', ha='center', va='bottom')\n",
        "plt.title('Original Distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(original_counts.index, ['No Fraud', 'Fraud'])\n",
        "\n",
        "# Downsampled class distribution\n",
        "plt.subplot(1, 2, 2)\n",
        "bars = plt.bar(downsampled_counts.index, downsampled_counts.values, color=['orange', 'green'])\n",
        "for bar, label in zip(bars, downsampled_percentages):\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5, f'{label:.2f}%', ha='center', va='bottom')\n",
        "plt.title('Downsampled Distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(downsampled_counts.index, ['No Fraud', 'Fraud'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "lr_predictions = lr_model.predict(X_test_scaled)\n",
        "print(\"Logistic Regression Results:\")\n",
        "print(classification_report(y_test, lr_predictions))\n",
        "print(f'Accuracy: {accuracy_score(y_test, lr_predictions) * 100:.2f}%')\n",
        "\n",
        "# Support Vector Machine (SVC)\n",
        "\n",
        "svc_model = LinearSVC()\n",
        "svc_model.fit(X_train_scaled, y_train)\n",
        "svc_predictions = svc_model.predict(X_test_scaled)\n",
        "print(\"SVC Model Results:\")\n",
        "print(classification_report(y_test, svc_predictions))\n",
        "print(f'Accuracy: {accuracy_score(y_test, svc_predictions) * 100:.2f}%')\n",
        "\n",
        "# Gaussian Naive Bayes\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_scaled, y_train)\n",
        "nb_predictions = nb_model.predict(X_test_scaled)\n",
        "print(\"Naive Bayes Results:\")\n",
        "print(classification_report(y_test, nb_predictions))\n",
        "print(f'Accuracy: {accuracy_score(y_test, nb_predictions) * 100:.2f}%')\n",
        "\n",
        "# Decision Tree Classifier\n",
        "\n",
        "dt_model = DecisionTreeClassifier(max_depth=1, random_state=0)\n",
        "dt_model.fit(X_train_scaled, y_train)\n",
        "dt_predictions = dt_model.predict(X_test_scaled)\n",
        "print(\"Decision Tree Results:\")\n",
        "print(classification_report(y_test, dt_predictions))\n",
        "print(f'Accuracy: {accuracy_score(y_test, dt_predictions) * 100:.2f}%')\n",
        "\n",
        "# Random Forest Classifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "rf_predictions = rf_model.predict(X_test_scaled)\n",
        "print(\"Random Forest Results:\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "print(f'Accuracy: {accuracy_score(y_test, rf_predictions) * 100:.2f}%')\n",
        "\n",
        "# XGBoost Classifier\n",
        "\n",
        "xgb_model = XGBClassifier(random_state=0)\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "xgb_predictions = xgb_model.predict(X_test_scaled)\n",
        "print(\"XGBoost Results:\")\n",
        "print(classification_report(y_test, xgb_predictions))\n",
        "print(f'Accuracy: {accuracy_score(y_test, xgb_predictions) * 100:.2f}%')\n",
        "\n",
        "# Accuracy Comparison\n",
        "\n",
        "models = ['XGBoost', 'Random Forest', 'Decision Tree', 'Logistic Regression', 'SVC', 'Naive Bayes']\n",
        "accuracies = [accuracy_score(y_test, xgb_predictions), accuracy_score(y_test, rf_predictions),\n",
        "              accuracy_score(y_test, dt_predictions), accuracy_score(y_test, lr_predictions),\n",
        "              accuracy_score(y_test, svc_predictions), accuracy_score(y_test, nb_predictions)]\n",
        "\n",
        "results_df = pd.DataFrame({'Model': models, 'Accuracy': accuracies})\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.bar(results_df['Model'], results_df['Accuracy'], color='skyblue')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}